# Homework-5 [Name: Avinash ]
# Transformer Core Components: Attention & Encoder Implementation

This project implements two fundamental building blocks of the Transformer architecture (as introduced in *Attention Is All You Need*):
1.  **Scaled Dot-Product Attention** (implemented from scratch using NumPy).
2.  **Transformer Encoder Block** (implemented using PyTorch `nn.Module`).

## Prerequisites

To run these scripts, ensure you have the following dependencies installed:

```bash
pip install numpy torch
