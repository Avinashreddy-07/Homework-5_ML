{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLQwuJdHqf1mN22ZSNpTgj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avinashreddy-07/Homework-5_ML/blob/main/Homework-5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question -1"
      ],
      "metadata": {
        "id": "TN45glNnD7qX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ky9H-GHDZ68",
        "outputId": "aa914d03-1f50-4619-d939-a3a1c6b45193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.29848396 0.34158447 0.35993157]\n",
            " [0.32124848 0.28982088 0.38893064]\n",
            " [0.27423777 0.3807327  0.34502952]]\n",
            "Context Vector:\n",
            " [[0.45414656 0.62930578 0.88446539 0.55907972]\n",
            " [0.42634493 0.61489103 0.88839965 0.55909714]\n",
            " [0.47503994 0.63816909 0.88103249 0.56035685]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Q, K, V: numpy arrays of shape (seq_len, d_k)\n",
        "    Returns: attention_weights, context_vector\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Compute raw attention scores: QK^T\n",
        "    scores = np.dot(Q, K.T)\n",
        "\n",
        "    # 2. Scale by sqrt(d_k)\n",
        "    d_k = Q.shape[-1]\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "\n",
        "    # 3. Apply softmax row-wise\n",
        "    # subtract max for numerical stability\n",
        "    exp_scores = np.exp(scaled_scores - np.max(scaled_scores, axis=-1, keepdims=True))\n",
        "    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
        "\n",
        "    # 4. Multiply weights with V\n",
        "    context_vector = np.dot(attention_weights, V)\n",
        "\n",
        "    return attention_weights, context_vector\n",
        "\n",
        "\n",
        "# Example usage\n",
        "Q = np.random.rand(3, 4)  # seq=3, d_k=4\n",
        "K = np.random.rand(3, 4)\n",
        "V = np.random.rand(3, 4)\n",
        "\n",
        "attn_wts, ctx = scaled_dot_product_attention(Q, K, V)\n",
        "print(\"Attention Weights:\\n\", attn_wts)\n",
        "print(\"Context Vector:\\n\", ctx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question -2"
      ],
      "metadata": {
        "id": "ULpFK_FJFqrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleTransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model=128, num_heads=8, ff_dim=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # (a) Multi-head self-attention\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
        "\n",
        "        # LayerNorm for residuals\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # (b) Feed-forward network\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, d_model)\n",
        "\n",
        "        # 1) Self-attention\n",
        "        attn_output, _ = self.attn(x, x, x)    # Q=K=V=x\n",
        "\n",
        "        # 2) Add & Norm\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        # 3) Feed-forward network\n",
        "        ff_output = self.ff(x)\n",
        "\n",
        "        # 4) Add & Norm\n",
        "        x = self.norm2(x + ff_output)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "POW6KmCqDjYV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleTransformerEncoder(d_model=128, num_heads=8)\n",
        "\n",
        "x = torch.randn(32, 10, 128)     # (batch, seq_len, d_model)\n",
        "\n",
        "out = model(x)\n",
        "print(\"Output shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yph5qewtDn-t",
        "outputId": "0c3256f8-1a02-4fee-9e74-d8ec9fa8f7f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([32, 10, 128])\n"
          ]
        }
      ]
    }
  ]
}